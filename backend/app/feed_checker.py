import re
import requests
from collections import Counter, defaultdict
from enum import Enum
from lxml import etree
from io import BytesIO
import logging
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime

from app.exceptions import FeedDownloadError, FeedValidationError

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ main.py, –ø—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∞–µ–º logger
logger = logging.getLogger(__name__)

class ProblemType(Enum):
    MISSING_ID = "–±–µ–∑ —Ç–µ–≥–∞ <id> –∏–ª–∏ —Å –ø—É—Å—Ç—ã–º <id>"
    MISSING_AVAILABLE = "–±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ <available> –∏–ª–∏ —Å –ø—É—Å—Ç—ã–º <available>"
    MISSING_NAME = "–±–µ–∑ —Ç—ç–≥–∞ <name>, –ª–∏–±–æ –±–µ–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ <typePrefix> + <vendor> + <model>"
    MISSING_LINK = "–±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É —Ç–æ–≤–∞—Ä–∞"
    PRICE_ISSUES = "—Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π, —Ä–∞–≤–Ω–æ–π –Ω—É–ª—é –∏–ª–∏ –±–µ–∑ —Ü–µ–Ω—ã"
    MISSING_CATEGORY = "–±–µ–∑ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
    INVALID_CATEGORY = "—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–µ—Ä–µ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
    MULTIPLE_CATEGORIES = "–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, —Å–ø–∏—Å–æ–∫ –∏—Ö categoryId –Ω–µ –æ–±—ë—Ä–Ω—É—Ç –≤ —Ç–µ–≥ <categories>"
    MISSING_VENDOR = "–±–µ–∑ —Ç—ç–≥–∞ <vendor>, —Å –ø—É—Å—Ç—ã–º <vendor> –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –ø–ª–æ—Ö–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (Null, –ë–µ–∑ –±—Ä–µ–Ω–¥–∞, –ù–µ—Ç, –ù–µ —É–∫–∞–∑–∞–Ω –∏ —Ç.–ø.)"
    MISSING_IMAGE = "–±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞"

class FeedChecker:
    def __init__(self, site_id: int, site_url: str = None, file_content: bytes = None, progress_callback=None):
        self.site_id = site_id
        self.site_url = site_url
        self.file_content = file_content
        self.progress_callback = progress_callback
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/xml, text/xml, */*; q=0.01',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º Brotli
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        self.feed_tree = None
        self.site_offers = None
        self.site_categories = None
        self.available_offers_count = 0
        self.unavailable_offers_count = 0
        self.empty_availability_count = 0
        self.offers_without_id = []
        self.duplicate_ids = []
        self.offers_without_name = []
        self.offers_without_link = []
        self.offers_price_issues = []
        self.offers_without_category = []
        self.offers_invalid_category = []
        self.offers_multiple_categories = []
        self.offers_vendor_issues = []
        self.offers_without_image = []
        self.offers_without_availability = []
        self.brands = set()
        self.categories_ids = []
        self.categories_names = []
        self.categories_full_info = []
        self.empty_categories = []
        self.duplicated_cats = []
        self.dual_categories = []
        self.invalid_param_errors = []  # –û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.duplicate_param_errors = []  # –û—à–∏–±–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

    def get_url_text(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–¥–∞ –ø–æ URL —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        site_data = None
        try:
            logger.info(f"üåê Starting download from: {self.site_url}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
            session = requests.Session()
            
            # –î–æ–±–∞–≤–ª—è–µ–º Referer –∑–∞–≥–æ–ª–æ–≤–æ–∫, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
            from urllib.parse import urlparse
            parsed_url = urlparse(self.site_url)
            referer_url = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            request_headers = {**self.headers, 'Referer': referer_url}
            logger.info(f"üìã Using Referer: {referer_url}")
            
            # –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è cookies (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            # –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å, –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ—Å—Å–∏—é
            try:
                logger.info(f"üç™ Getting initial cookies from main page...")
                main_page_response = session.get(referer_url, headers=self.headers, timeout=10, allow_redirects=True)
                logger.info(f"‚úÖ Got cookies, status: {main_page_response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not get initial cookies: {e}, continuing anyway...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –∑–∞–ø—Ä–æ—Å–∞
            # –î–ª—è PHP —Ñ–∞–π–ª–æ–≤ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º GET, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - POST
            url_request = None
            if self.site_url[-3:].lower() == 'php':
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º GET
                try:
                    logger.info(f"üì° Trying GET request for PHP file...")
                    url_request = session.get(self.site_url, headers=request_headers, timeout=300, stream=True, allow_redirects=True)
                    url_request.raise_for_status()
                    logger.info(f"‚úÖ GET request successful: status={url_request.status_code}")
                except requests.exceptions.HTTPError as e:
                    if e.response and e.response.status_code == 405:
                        # –ï—Å–ª–∏ GET –Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω, –ø—Ä–æ–±—É–µ–º POST
                        logger.info(f"‚ö†Ô∏è GET returned 405, trying POST...")
                        url_request = session.post(self.site_url, headers=request_headers, timeout=300, stream=True, allow_redirects=True)
                        url_request.raise_for_status()
                        logger.info(f"‚úÖ POST request successful: status={url_request.status_code}")
                    else:
                        raise
            else:
                # –î–ª—è –Ω–µ-PHP —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º GET
                logger.info(f"üì° Making GET request...")
                url_request = session.get(self.site_url, headers=request_headers, timeout=300, stream=True, allow_redirects=True)
                url_request.raise_for_status()
            logger.info(f"‚úÖ Got response: status={url_request.status_code}, headers={dict(url_request.headers)}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            total_size = int(url_request.headers.get('content-length', 0))
            logger.info(f"üì¶ Content-Length: {total_size} bytes ({total_size / 1024 / 1024:.2f} MB)" if total_size > 0 else "‚ö†Ô∏è Content-Length not provided by server")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
            loaded_size = 0
            chunks = []
            chunk_count = 0
            
            logger.info(f"üîΩ Starting to download chunks...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type –∑–∞–≥–æ–ª–æ–≤–æ–∫
            content_type = url_request.headers.get('Content-Type', '').lower()
            logger.info(f"üìã Content-Type: {content_type}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ chunks
            # –î–ª—è Brotli –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π
            for chunk in url_request.iter_content(chunk_size=8192):
                if chunk:
                    chunks.append(chunk)
                    loaded_size += len(chunk)
                    chunk_count += 1
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 100-–π chunk (–∫–∞–∂–¥—ã–µ ~800KB)
                    if chunk_count % 100 == 0:
                        logger.info(f"üì• Downloaded {chunk_count} chunks, {loaded_size / 1024 / 1024:.2f} MB")
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –µ—Å—Ç—å callback
                    if self.progress_callback and total_size > 0:
                        self.progress_callback(loaded_size, total_size)
            
            logger.info(f"‚úÖ Download complete: {chunk_count} chunks, {loaded_size / 1024 / 1024:.2f} MB total")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Encoding –∑–∞–≥–æ–ª–æ–≤–æ–∫ –î–û –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è chunks
            content_encoding = url_request.headers.get('Content-Encoding', '').lower()
            logger.info(f"üì¶ Content-Encoding: {content_encoding}")
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ chunks
            total_chunks_size = sum(len(c) for c in chunks)
            logger.info(f"üìä Chunks info: count={len(chunks)}, total_size={total_chunks_size / 1024 / 1024:.2f} MB")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            logger.info(f"üîç About to process content encoding: {content_encoding}")
            logger.info(f"üîç Memory check: chunks ready={len(chunks) > 0}")
            
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–∂–∞—Ç—ã Brotli, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—É—é –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—é
            # –î–ª—è –í–°–ï–• —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—É—é –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—é –∏–∑ chunks
            # –≠—Ç–æ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ –ø–∞–º—è—Ç–∏, —Ç–∞–∫ –∫–∞–∫ –º—ã –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å
            # –∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É —á–µ—Ä–µ–∑ url_request.content
            if content_encoding == 'br':
                logger.info(f"üîÑ Content is Brotli compressed ({total_chunks_size / 1024 / 1024:.2f} MB)")
                logger.info(f"üîÑ Using manual decompression from chunks (memory efficient)...")
                logger.info(f"üìä Chunks count: {len(chunks)}, total size: {total_chunks_size / 1024 / 1024:.2f} MB")
                
                try:
                    import brotli
                    logger.info(f"‚úÖ Brotli library imported successfully")
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∂–∞—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ chunks
                    logger.info(f"üî® Joining {len(chunks)} chunks into single byte array...")
                    logger.info(f"‚è±Ô∏è Join operation started at: {datetime.now().isoformat()}")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º BytesIO –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                    buffer = BytesIO()
                    for i, chunk in enumerate(chunks):
                        buffer.write(chunk)
                        if (i + 1) % 500 == 0:
                            logger.info(f"üìù Joined {i + 1}/{len(chunks)} chunks...")
                    compressed_bytes = buffer.getvalue()
                    buffer.close()
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º chunks —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                    chunks.clear()
                    del chunks
                    
                    logger.info(f"‚úÖ Join complete")
                    logger.info(f"üì¶ Compressed data size: {len(compressed_bytes) / 1024 / 1024:.2f} MB")
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º compressed_bytes –ø–æ—Å–ª–µ –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏–∏
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—Ä–æ–≤–∞–Ω—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ
                        logger.info(f"üîç Checking if data is already decompressed...")
                        first_bytes_check = compressed_bytes[:100].decode('utf-8', errors='ignore')
                        if first_bytes_check.strip().startswith('<'):
                            logger.info(f"‚úÖ Data appears to be already decompressed by requests")
                            content_bytes = compressed_bytes
                            del compressed_bytes  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                        else:
                            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—Ä–æ–≤–∞—Ç—å
                            logger.info(f"üîÑ Starting Brotli decompression of {len(compressed_bytes) / 1024 / 1024:.2f} MB...")
                            logger.info(f"‚è±Ô∏è Decompression started at: {datetime.now().isoformat()}")
                            
                            start_time = time.time()
                            content_bytes = brotli.decompress(compressed_bytes)
                            
                            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º compressed_bytes —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏–∏
                            del compressed_bytes
                            
                            elapsed_time = time.time() - start_time
                            logger.info(f"‚úÖ Brotli decompression successful!")
                            logger.info(f"üìä Decompressed size: {len(content_bytes) / 1024 / 1024:.2f} MB")
                            logger.info(f"‚è±Ô∏è Decompression took: {elapsed_time:.2f} seconds")
                            logger.info(f"‚è±Ô∏è Decompression speed: {len(content_bytes) / elapsed_time / 1024 / 1024:.2f} MB/s")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            first_bytes = content_bytes[:100].decode('utf-8', errors='ignore')
                            if first_bytes.strip().startswith('<'):
                                logger.info(f"‚úÖ Content starts with '<' - looks like valid XML")
                            else:
                                logger.warning(f"‚ö†Ô∏è Content doesn't start with '<', but continuing...")
                                
                    except Exception as decomp_error:
                        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                        if 'compressed_bytes' in locals():
                            del compressed_bytes
                        
                        logger.error(f"‚ùå Brotli decompression failed: {decomp_error}")
                        logger.error(f"‚ùå Error type: {type(decomp_error).__name__}")
                        import traceback
                        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
                        
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—Å–∏—Ç—å –±–µ–∑ stream
                        logger.info(f"üîÑ Last attempt: retrying without stream for auto-decompression...")
                        url_request.close()
                        if self.site_url[-3:].lower() == 'php':
                            try:
                                url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                                url_request.raise_for_status()
                            except requests.exceptions.HTTPError as e:
                                if e.response and e.response.status_code == 405:
                                    url_request = session.post(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                                    url_request.raise_for_status()
                                else:
                                    raise
                        else:
                            url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                            url_request.raise_for_status()
                        
                        content_bytes = url_request.content
                        logger.info(f"‚úÖ Got content from retry: {len(content_bytes) / 1024 / 1024:.2f} MB")
                
                except ImportError:
                    logger.error(f"‚ùå brotli library not installed")
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º chunks
                    chunks.clear()
                    del chunks
                    
                    # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∑–∞–ø—Ä–æ—Å–∏—Ç—å –±–µ–∑ stream
                    logger.info(f"üîÑ Retrying without stream for auto-decompression...")
                    url_request.close()
                    if self.site_url[-3:].lower() == 'php':
                        try:
                            url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                            url_request.raise_for_status()
                        except requests.exceptions.HTTPError as e:
                            if e.response and e.response.status_code == 405:
                                url_request = session.post(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                                url_request.raise_for_status()
                            else:
                                raise
                    else:
                        url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                        url_request.raise_for_status()
                    
                    content_bytes = url_request.content
                    logger.info(f"‚úÖ Got content from retry (no brotli lib): {len(content_bytes) / 1024 / 1024:.2f} MB")
                except Exception as e:
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    if 'chunks' in locals():
                        chunks.clear()
                        del chunks
                    if 'compressed_bytes' in locals():
                        del compressed_bytes
                        
                    logger.error(f"‚ùå Brotli handling failed: {e}")
                    logger.error(f"‚ùå Error type: {type(e).__name__}")
                    import traceback
                    logger.error(f"‚ùå Full traceback: {traceback.format_exc()}")
                    
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    try:
                        logger.info(f"üîÑ Last attempt: retrying without stream...")
                        url_request.close()
                        if self.site_url[-3:].lower() == 'php':
                            try:
                                url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                                url_request.raise_for_status()
                            except requests.exceptions.HTTPError as e:
                                if e.response and e.response.status_code == 405:
                                    url_request = session.post(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                                    url_request.raise_for_status()
                                else:
                                    raise
                        else:
                            url_request = session.get(self.site_url, headers=request_headers, timeout=600, allow_redirects=True)
                            url_request.raise_for_status()
                        
                        content_bytes = url_request.content
                        logger.info(f"‚úÖ Got content from last retry: {len(content_bytes) / 1024 / 1024:.2f} MB")
                    except Exception as retry_error:
                        logger.error(f"‚ùå Last retry also failed: {retry_error}")
                        raise FeedDownloadError(
                            message=f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (Brotli): {str(e)}",
                            error_code="DECOMPRESSION_ERROR",
                            url=self.site_url,
                            details={
                                "error_type": "DecompressionError",
                                "content_encoding": content_encoding,
                                "technical_message": str(e),
                                "retry_error": str(retry_error),
                                "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ brotli: pip install brotli"
                            }
                        )
            elif content_encoding in ('gzip', 'deflate'):
                # –î–ª—è gzip/deflate –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—é requests
                logger.info(f"üîÑ Content is compressed ({content_encoding}), using auto-decompression...")
                url_request.close()
                if self.site_url[-3:].lower() == 'php':
                    try:
                        url_request = session.get(self.site_url, headers=request_headers, timeout=300, allow_redirects=True)
                        url_request.raise_for_status()
                    except requests.exceptions.HTTPError as e:
                        if e.response and e.response.status_code == 405:
                            url_request = session.post(self.site_url, headers=request_headers, timeout=300, allow_redirects=True)
                            url_request.raise_for_status()
                        else:
                            raise
                else:
                    url_request = session.get(self.site_url, headers=request_headers, timeout=300, allow_redirects=True)
                    url_request.raise_for_status()
                content_bytes = url_request.content
                logger.info(f"‚úÖ Got auto-decompressed content: {len(content_bytes)} bytes")
            else:
                # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ chunks (–Ω–µ —Å–∂–∞—Ç–æ)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ chunks —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–æ–π
                if not chunks or len(chunks) == 0:
                    logger.error(f"‚ùå No chunks available for uncompressed content")
                    raise FeedDownloadError(
                        message="–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω",
                        error_code="EMPTY_CONTENT",
                        url=self.site_url,
                        details={"chunks_count": 0}
                    )
                
                # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º BytesIO
                if total_chunks_size > 100 * 1024 * 1024:  # > 100 MB
                    logger.info(f"‚ö†Ô∏è Large uncompressed file ({total_chunks_size / 1024 / 1024:.2f} MB), using BytesIO...")
                    buffer = BytesIO()
                    for i, chunk in enumerate(chunks):
                        buffer.write(chunk)
                        if (i + 1) % 500 == 0:
                            logger.info(f"üìù Joined {i + 1}/{len(chunks)} chunks...")
                    content_bytes = buffer.getvalue()
                    buffer.close()
                    logger.info(f"‚úÖ Joined {len(chunks)} chunks using BytesIO: {len(content_bytes) / 1024 / 1024:.2f} MB")
                else:
                    content_bytes = b''.join(chunks)
                    logger.info(f"üî® Joined {len(chunks)} chunks: {len(content_bytes) / 1024 / 1024:.2f} MB")

            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç chunks (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ chunks —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –±—ã–ª —É–∂–µ —É–¥–∞–ª–µ–Ω
            try:
                if 'chunks' in locals() and chunks is not None:
                    chunks.clear()
                    del chunks
                    logger.info(f"üóëÔ∏è Cleared chunks from memory")
            except (NameError, UnboundLocalError):
                # chunks —É–∂–µ –±—ã–ª —É–¥–∞–ª–µ–Ω - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                pass
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            content_preview = content_bytes[:1000].decode('utf-8', errors='ignore')
            
            if 'UTF-8' in content_preview or 'utf-8' in content_preview:
                encoding = 'utf-8'
                site_data = content_bytes.decode('utf-8', errors='replace')
            elif 'windows-1251' in content_preview:
                encoding = 'cp1251'
                site_data = content_bytes.decode('cp1251', errors='replace')
            else:
                encoding = 'utf-8'
                site_data = content_bytes.decode('utf-8', errors='replace')
            
            logger.info(f"üìÑ Decoded with {encoding}, final text length: {len(site_data)} characters")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if site_data:
                logger.info(f"üìã First 200 chars of decoded content: {repr(site_data[:200])}")
                logger.info(f"üî§ First 50 bytes (hex): {site_data[:50].encode(encoding, errors='replace').hex()}")
                
                # –£–¥–∞–ª—è–µ–º BOM –µ—Å–ª–∏ –µ—Å—Ç—å
                if site_data.startswith('\ufeff'):  # UTF-8 BOM
                    logger.warning(f"‚ö†Ô∏è Found UTF-8 BOM, stripping it")
                    site_data = site_data[1:]
                elif site_data.startswith('\xef\xbb\xbf'):  # UTF-8 BOM in bytes (–µ—Å–ª–∏ –Ω–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–ª—Å—è)
                    logger.warning(f"‚ö†Ô∏è Found UTF-8 BOM (bytes), stripping it")
                    site_data = site_data[3:]
                
                # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
                site_data = site_data.lstrip()
            else:
                logger.error(f"‚ùå site_data is empty or None!")
            
            logger.info(f"üéâ get_url_text() completed successfully, returning {len(site_data)} characters")
            return site_data
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Connection error for {self.site_url}: {e}")
            raise FeedDownloadError(
                message=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ URL: {self.site_url}",
                error_code="CONNECTION_ERROR",
                url=self.site_url,
                details={
                    "error_type": "ConnectionError",
                    "technical_message": str(e),
                    "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∞–¥—Ä–µ—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏ —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω"
                }
            )
        except requests.exceptions.Timeout as e:
            logger.error(f"Timeout error for {self.site_url}: {e}")
            raise FeedDownloadError(
                message=f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∏–¥–∞ (300 —Å–µ–∫—É–Ω–¥)",
                error_code="TIMEOUT_ERROR",
                url=self.site_url,
                details={
                    "error_type": "Timeout",
                    "timeout_seconds": 300,
                    "technical_message": str(e),
                    "suggestion": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"
                }
            )
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else 0
            logger.error(f"HTTP error {status_code} for {self.site_url}: {e}")
            
            error_messages = {
                404: ("–§–∏–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω (404)", "NOT_FOUND", "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å URL"),
                403: ("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω (403)", "FORBIDDEN", "–í–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã"),
                401: ("–¢—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (401)", "UNAUTHORIZED", "–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∏–¥—É"),
                500: ("–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ (500)", "SERVER_ERROR", "–ü—Ä–æ–±–ª–µ–º–∞ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ —Å–µ—Ä–≤–µ—Ä–∞"),
                502: ("Bad Gateway (502)", "BAD_GATEWAY", "–°–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"),
                503: ("Service Unavailable (503)", "SERVICE_UNAVAILABLE", "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"),
            }
            
            if status_code in error_messages:
                msg, code, suggestion = error_messages[status_code]
            else:
                msg = f"–û—à–∏–±–∫–∞ HTTP {status_code}"
                code = f"HTTP_{status_code}"
                suggestion = "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞"
            
            raise FeedDownloadError(
                message=msg,
                error_code=code,
                url=self.site_url,
                status_code=status_code,
                details={
                    "error_type": "HTTPError",
                    "technical_message": str(e),
                    "suggestion": suggestion,
                    "response_headers": dict(e.response.headers) if e.response else {}
                }
            )
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {self.site_url}: {e}")
            raise FeedDownloadError(
                message=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∏–¥–∞",
                error_code="REQUEST_ERROR",
                url=self.site_url,
                details={
                    "error_type": type(e).__name__,
                    "technical_message": str(e),
                    "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞"
                }
            )
        
        return site_data

    def _get_error_context(self, content: str, line_num: int, context_lines: int = 3) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –≤–æ–∫—Ä—É–≥ –ø—Ä–æ–±–ª–µ–º–Ω–æ–π —Å—Ç—Ä–æ–∫–∏)"""
        lines = content.split('\n')
        start = max(0, line_num - context_lines - 1)
        end = min(len(lines), line_num + context_lines)
        
        context = []
        for i in range(start, end):
            context.append({
                "line_number": i + 1,
                "content": lines[i] if i < len(lines) else "",
                "is_error_line": i + 1 == line_num
            })
        
        return {
            "error_line": lines[line_num - 1] if 0 < line_num <= len(lines) else "",
            "context": context
        }
    
    def _truncate_line_with_context(self, line: str, error_column: int, max_length: int = 200, context_before: int = 50, context_after: int = 50) -> Dict:
        """–û–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É, –ø–æ–∫–∞–∑—ã–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –æ—à–∏–±–∫–∏"""
        if not line or len(line) <= max_length:
            return {
                "truncated": False,
                "full_line": line,
                "preview": line,
                "error_position": error_column
            }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ –¥–ª—è –ø–æ–∫–∞–∑–∞
        start_pos = max(0, error_column - context_before)
        end_pos = min(len(line), error_column + context_after)
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–∞—è, –æ–±—Ä–µ–∑–∞–µ–º
        if end_pos - start_pos > max_length:
            # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –≤–æ–∫—Ä—É–≥ –æ—à–∏–±–∫–∏
            start_pos = max(0, error_column - max_length // 2)
            end_pos = min(len(line), start_pos + max_length)
        
        preview = line[start_pos:end_pos]
        truncated_before = start_pos > 0
        truncated_after = end_pos < len(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –æ–±—Ä–µ–∑–∫–∏
        if truncated_before:
            preview = "..." + preview
        if truncated_after:
            preview = preview + "..."
        
        return {
            "truncated": True,
            "full_line": line,
            "preview": preview,
            "error_position": error_column - start_pos + (3 if truncated_before else 0),
            "start_pos": start_pos,
            "end_pos": end_pos,
            "line_length": len(line)
        }
    
    def _check_unescaped_ampersand(self, text: str, error_column: int = None) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–º–ø–µ—Ä—Å–∞–Ω–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text or "&" not in text:
            return False, ""
        
        import re
        # –í–∞–ª–∏–¥–Ω—ã–µ XML —Å—É—â–Ω–æ—Å—Ç–∏: &amp; &lt; &gt; &quot; &apos; –∏–ª–∏ &#—á–∏—Å–ª–æ; –∏–ª–∏ &#xhex;
        # –ò—â–µ–º –∞–º–ø–µ—Ä—Å–∞–Ω–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        # –ü–∞—Ç—Ç–µ—Ä–Ω: & –Ω–µ –∑–∞ –∫–æ—Ç–æ—Ä—ã–º —Å–ª–µ–¥—É–µ—Ç:
        # - amp; –∏–ª–∏ lt; –∏–ª–∏ gt; –∏–ª–∏ quot; –∏–ª–∏ apos; (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏)
        # - #—á–∏—Å–ª–æ; (—á–∏—Å–ª–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å)
        # - #xhex; (hex —Å—É—â–Ω–æ—Å—Ç—å)
        # - [a-zA-Z][a-zA-Z0-9]*; (–∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å)
        
        # –ò—â–µ–º –≤—Å–µ –∞–º–ø–µ—Ä—Å–∞–Ω–¥—ã
        ampersand_positions = [m.start() for m in re.finditer(r'&', text)]
        
        for pos in ampersand_positions:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–µ–¥—É–µ—Ç –ø–æ—Å–ª–µ &
            after_amp = text[pos + 1:pos + 100]  # –ë–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            if after_amp.startswith(('amp;', 'lt;', 'gt;', 'quot;', 'apos;')):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ &#—á–∏—Å–ª–æ; –∏–ª–∏ &#xhex;
            if after_amp.startswith('#'):
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω &#—á–∏—Å–ª–æ; –∏–ª–∏ &#xhex;
                if re.match(r'#\d+;', after_amp) or re.match(r'#x[0-9a-fA-F]+;', after_amp):
                    continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ [a-zA-Z][a-zA-Z0-9]*;
            if re.match(r'[a-zA-Z][a-zA-Z0-9]*;', after_amp):
                continue
            
            # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∞–º–ø–µ—Ä—Å–∞–Ω–¥–∞
            context_start = max(0, pos - 30)
            context_end = min(len(text), pos + 50)
            context = text[context_start:context_end]
            if context_start > 0:
                context = "..." + context
            if context_end < len(text):
                context = context + "..."
            
            hint = f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {pos + 1}. –ó–∞–º–µ–Ω–∏—Ç–µ & –Ω–∞ &amp;"
            if error_column and abs(pos - error_column) < 10:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Ä—è–¥–æ–º —Å –∞–º–ø–µ—Ä—Å–∞–Ω–¥–æ–º, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞
                hint = f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) - –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ &amp;"
            
            return True, hint
        
        return False, ""
    
    def _translate_xml_error(self, error_msg: str) -> str:
        """–ü–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É XML –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫"""
        translations = {
            "Opening and ending tag mismatch": "–û—Ç–∫—Ä—ã–≤–∞—é—â–∏–π –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π —Ç–µ–≥–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç",
            "Premature end of data": "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ - –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ –∑–∞–∫—Ä—ã—Ç—ã —Ç–µ–≥–∏",
            "Extra content at the end": "–õ–∏—à–Ω–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ —Ç–µ–≥–∞",
            "attributes construct error": "–û—à–∏–±–∫–∞ –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö —Ç–µ–≥–∞",
            "expected '>'": "–û–∂–∏–¥–∞–µ—Ç—Å—è —Å–∏–º–≤–æ–ª '>' –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è —Ç–µ–≥–∞",
            "expected '<'": "–û–∂–∏–¥–∞–µ—Ç—Å—è —Å–∏–º–≤–æ–ª '<' –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è —Ç–µ–≥–∞",
            "Blank needed here": "–¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–±–µ–ª",
            "Entity": "–ù–µ–≤–µ—Ä–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å",
            "not defined": "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
            "Specification mandates value": "–ó–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–∞–≤—ã—á–∫–∞—Ö",
            "Couldn't find end of Start Tag": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–∫–æ–Ω—á–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–≥–æ —Ç–µ–≥–∞",
            "StartTag: invalid element name": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–º—è —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–º —Ç–µ–≥–µ",
            "xmlParseEntityRef: no name": "–û—à–∏–±–∫–∞ –≤ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–µ (&) - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ &amp; –≤–º–µ—Å—Ç–æ &",
            "AttValue": "–û—à–∏–±–∫–∞ –≤ –∑–Ω–∞—á–µ–Ω–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–∞",
            "Char 0x0 out of allowed range": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Å–∏–º–≤–æ–ª (–Ω—É–ª–µ–≤–æ–π –±–∞–π—Ç)",
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å –∞–º–ø–µ—Ä—Å–∞–Ω–¥–æ–º (EntityRef, Entity –∏ —Ç.–¥.)
        error_lower = error_msg.lower()
        if ("entityref" in error_lower or "entity" in error_lower) and ("expecting" in error_lower or ";" in error_lower or "no name" in error_lower):
            return "–ù–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) - –∑–∞–º–µ–Ω–∏—Ç–µ & –Ω–∞ &amp;"
        if "Entity" in error_msg or "entity" in error_msg.lower() or ("&" in error_msg and ("expecting" in error_lower or ";" in error_lower)):
            if "no name" in error_lower or "not defined" in error_lower or "expecting" in error_lower:
                return "–ù–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) - –∑–∞–º–µ–Ω–∏—Ç–µ & –Ω–∞ &amp;"
            return "–ù–µ–≤–µ—Ä–Ω–∞—è XML-—Å—É—â–Ω–æ—Å—Ç—å (–ø—Ä–æ–±–ª–µ–º–∞ —Å –∞–º–ø–µ—Ä—Å–∞–Ω–¥–æ–º &) - –∑–∞–º–µ–Ω–∏—Ç–µ & –Ω–∞ &amp;"
        
        for eng, rus in translations.items():
            if eng.lower() in error_msg.lower():
                return rus
        
        return error_msg
    
    def check_xml_syntax(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ XML —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–∞—Ö"""
        try:
            content_str = None
            all_errors = []  # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏
            
            if self.file_content:
                content_str = self.file_content.decode('utf-8', errors='replace')
                # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å —Å recover=True –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö –æ—à–∏–±–æ–∫
                parser = etree.XMLParser(recover=True)
                try:
                    etree.fromstring(self.file_content, parser=parser)
                except etree.XMLSyntaxError:
                    pass  # –û—à–∏–±–∫–∏ –±—É–¥—É—Ç –≤ error_log
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏–∑ error_log
                if parser.error_log:
                    for error_entry in parser.error_log:
                        all_errors.append({
                            "line": error_entry.line,
                            "column": error_entry.column,
                            "message": str(error_entry.message)
                        })
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏, –ø—Ä–æ–±—É–µ–º –±–µ–∑ recover –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏
                if all_errors:
                    etree.fromstring(self.file_content)  # –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
            else:
                site_data = self.get_url_text()
                content_str = site_data
                
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"üìã Content preview (first 500 chars): {content_str[:500]}")
                logger.info(f"üìè Content length: {len(content_str)} characters")
                logger.info(f"üî§ First 50 bytes (hex): {content_str[:50].encode('utf-8').hex()}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–æ–π –ª–∏ —Ñ–∞–π–ª
                if not content_str or not content_str.strip():
                    raise FeedDownloadError(
                        message="–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞",
                        error_code="EMPTY_RESPONSE",
                        url=self.site_url if hasattr(self, 'site_url') else None,
                        details={
                            "error_type": "EmptyResponse",
                            "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL —Ñ–∏–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ."
                        }
                    )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å '<'
                stripped = content_str.strip()
                if not stripped.startswith('<'):
                    logger.error(f"‚ùå Content does not start with '<'. First 100 chars: {repr(stripped[:100])}")
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —á—Ç–æ —ç—Ç–æ
                    if stripped.startswith('<!doctype') or stripped.lower().startswith('<html'):
                        raise FeedDownloadError(
                            message="–°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–º–µ—Å—Ç–æ XML —Ñ–∏–¥–∞",
                            error_code="INVALID_CONTENT_TYPE",
                            url=self.site_url if hasattr(self, 'site_url') else None,
                            details={
                                "error_type": "HTMLResponse",
                                "content_preview": stripped[:500],
                                "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL —Ñ–∏–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏."
                            }
                        )
                    else:
                        raise FeedDownloadError(
                            message=f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞. –û–∂–∏–¥–∞–µ—Ç—Å—è XML, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å '<', –ø–æ–ª—É—á–µ–Ω–æ: {repr(stripped[:50])}",
                            error_code="INVALID_FORMAT",
                            url=self.site_url if hasattr(self, 'site_url') else None,
                            details={
                                "error_type": "InvalidFormat",
                                "content_preview": stripped[:500],
                                "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL —Ñ–∏–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."
                            }
                        )
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                if 'UTF-8' in site_data or 'utf-8' in site_data:
                    encoding = 'utf-8'
                elif 'windows-1251' in site_data:
                    encoding = 'cp1251'
                else:
                    encoding = 'utf-8'
                
                xml_bytes = bytes(site_data, encoding=encoding)
                
                # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å —Å recover=True –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö –æ—à–∏–±–æ–∫
                parser = etree.XMLParser(recover=True)
                try:
                    etree.fromstring(xml_bytes, parser=parser)
                except etree.XMLSyntaxError:
                    pass  # –û—à–∏–±–∫–∏ –±—É–¥—É—Ç –≤ error_log
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏–∑ error_log
                if parser.error_log:
                    for error_entry in parser.error_log:
                        all_errors.append({
                            "line": error_entry.line,
                            "column": error_entry.column,
                            "message": str(error_entry.message)
                        })
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏, –ø—Ä–æ–±—É–µ–º –±–µ–∑ recover –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏
                if all_errors:
                    etree.fromstring(xml_bytes)  # –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
            
            # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ –±–µ–∑ –æ—à–∏–±–æ–∫
            if not all_errors:
                return {
                    "valid": True, 
                    "message": "XML –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω, —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                    "human_message": "‚úÖ –§–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω"
                }
            
        except etree.XMLSyntaxError as err:
            logging.error(f"XML Syntax Error: {err}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ HTML –ª–∏ —ç—Ç–æ
            if content_str:
                content_lower = content_str[:500].strip().lower()
                is_html_response = (
                    content_lower.startswith('<!doctype') or 
                    content_lower.startswith('<html') or 
                    '<body' in content_lower[:200]
                )
                
                if is_html_response:
                    # –≠—Ç–æ HTML, –∞ –Ω–µ XML
                    logger.error(f"‚ùå Server returned HTML instead of XML feed")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ –∏–∑ HTML
                    error_info = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"
                    import re
                    # –ò—â–µ–º title
                    error_match = re.search(r'<title[^>]*>(.*?)</title>', content_str[:1000], re.IGNORECASE | re.DOTALL)
                    if error_match:
                        error_info = error_match.group(1).strip()[:100]
                    # –ò—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
                    if not error_info or error_info == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞":
                        error_match = re.search(r'<h1[^>]*>(.*?)</h1>', content_str[:1000], re.IGNORECASE | re.DOTALL)
                        if error_match:
                            error_info = error_match.group(1).strip()[:100]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–º –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç JavaScript
                    requires_js = 'javascript' in content_lower or 'script' in content_lower[:500]
                    has_redirect = 'location' in content_lower or 'redirect' in content_lower[:500]
                    
                    suggestion = "–°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–º–µ—Å—Ç–æ XML —Ñ–∏–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏."
                    if requires_js:
                        suggestion += " –°–µ—Ä–≤–µ—Ä –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JavaScript."
                    if has_redirect:
                        suggestion += " –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ä–µ–¥–∏—Ä–µ–∫—Ç."
                    
                    raise FeedDownloadError(
                        message="–°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–º–µ—Å—Ç–æ XML —Ñ–∏–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ URL –Ω–µ–≤–µ—Ä–µ–Ω.",
                        error_code="INVALID_CONTENT_TYPE",
                        url=self.site_url if hasattr(self, 'site_url') else None,
                        details={
                            "error_type": "HTMLResponse",
                            "content_preview": content_str[:500],
                            "html_error": error_info,
                            "requires_javascript": requires_js,
                            "has_redirect": has_redirect,
                            "suggestion": suggestion
                        }
                    )
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏–∑ error_log, –µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã
            if not all_errors:
                error_log = err.error_log
                if error_log:
                    for error_entry in error_log:
                        all_errors.append({
                            "line": error_entry.line,
                            "column": error_entry.column,
                            "message": str(error_entry.message)
                        })
            
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∏ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            if not all_errors:
                error_log = err.error_log
                error_entry = error_log[0] if error_log else None
                if error_entry:
                    all_errors.append({
                        "line": error_entry.line,
                        "column": error_entry.column,
                        "message": str(error_entry.message)
                    })
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—É—é –æ—à–∏–±–∫—É –¥–µ—Ç–∞–ª—å–Ω–æ
            first_error = all_errors[0] if all_errors else {}
            line_num = first_error.get("line", 0)
            column = first_error.get("column", 0)
            error_msg = first_error.get("message", str(err))
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏
            context_info = {}
            if content_str and line_num > 0:
                context_info = self._get_error_context(content_str, line_num)
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –æ—à–∏–±–∫—É –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            human_message = self._translate_xml_error(error_msg)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä–æ–∫—É —Å –æ—à–∏–±–∫–æ–π
            error_line_full = ""
            if content_str and line_num > 0:
                lines = content_str.split('\n')
                if 0 < line_num <= len(lines):
                    error_line_full = lines[line_num - 1]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ - –µ—Å–ª–∏ —ç—Ç–æ EntityRef, —Ç–æ —Ç–æ—á–Ω–æ –∞–º–ø–µ—Ä—Å–∞–Ω–¥
            is_entity_ref_error = "EntityRef" in error_msg or "entityref" in error_msg.lower() or ("entity" in error_msg.lower() and "expecting" in error_msg.lower())
            has_ampersand, ampersand_hint = self._check_unescaped_ampersand(error_line_full, column)
            
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å EntityRef, –Ω–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –Ω–∞—à–ª–∞ –∞–º–ø–µ—Ä—Å–∞–Ω–¥, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∞–º–ø–µ—Ä—Å–∞–Ω–¥
            if is_entity_ref_error and not has_ampersand:
                has_ampersand = True
                ampersand_hint = " –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) - –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ &amp;"
                logger.info(f"üîç EntityRef error detected, marking as ampersand error. Line: {line_num}, Column: {column}")
            elif has_ampersand:
                ampersand_hint = f" {ampersand_hint}"
                logger.info(f"üîç Ampersand detected in line. Line: {line_num}, Column: {column}")
            else:
                ampersand_hint = ""
            
            logger.info(f"üìä Error processing: line={line_num}, column={column}, has_ampersand={has_ampersand}, is_entity_ref={is_entity_ref_error}, error_msg={error_msg[:100]}")
            
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            line_info = self._truncate_line_with_context(error_line_full, column if column > 0 else 0)
            error_line_display = line_info["preview"] if line_info["truncated"] else error_line_full
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –æ—à–∏–±–æ–∫
            detailed_errors = []
            for error in all_errors:
                err_line = error.get("line", 0)
                err_col = error.get("column", 0)
                err_msg = error.get("message", "")
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è —ç—Ç–æ–π –æ—à–∏–±–∫–∏
                err_line_full = ""
                if content_str and err_line > 0:
                    lines = content_str.split('\n')
                    if 0 < err_line <= len(lines):
                        err_line_full = lines[err_line - 1]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–º–ø–µ—Ä—Å–∞–Ω–¥
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ - –µ—Å–ª–∏ —ç—Ç–æ EntityRef, —Ç–æ —Ç–æ—á–Ω–æ –∞–º–ø–µ—Ä—Å–∞–Ω–¥
                is_entity_ref = "EntityRef" in err_msg or "entityref" in err_msg.lower() or ("entity" in err_msg.lower() and "expecting" in err_msg.lower())
                has_amp, err_ampersand_hint = self._check_unescaped_ampersand(err_line_full, err_col)
                
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å EntityRef, –Ω–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –Ω–∞—à–ª–∞ –∞–º–ø–µ—Ä—Å–∞–Ω–¥, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∞–º–ø–µ—Ä—Å–∞–Ω–¥
                if is_entity_ref and not has_amp:
                    has_amp = True
                    err_ampersand_hint = " –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–º–ø–µ—Ä—Å–∞–Ω–¥ (&) - –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ &amp;"
                elif has_amp:
                    err_ampersand_hint = f" {err_ampersand_hint}"
                else:
                    err_ampersand_hint = ""
                
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                err_line_info = self._truncate_line_with_context(err_line_full, err_col if err_col > 0 else 0)
                err_line_display = err_line_info["preview"] if err_line_info["truncated"] else err_line_full
                
                translated = self._translate_xml_error(err_msg)
                if err_line > 0:
                    if err_line_display:
                        detailed_msg = f"–°—Ç—Ä–æ–∫–∞ {err_line}, –ø–æ–∑–∏—Ü–∏—è {err_col}: {translated}{err_ampersand_hint}\n–ü—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {err_line_display}"
                        if err_line_info["truncated"]:
                            detailed_msg += f"\n(–°—Ç—Ä–æ–∫–∞ –æ–±—Ä–µ–∑–∞–Ω–∞, –ø–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞: {err_line_info['line_length']} —Å–∏–º–≤–æ–ª–æ–≤)"
                    else:
                        detailed_msg = f"–°—Ç—Ä–æ–∫–∞ {err_line}, –ø–æ–∑–∏—Ü–∏—è {err_col}: {translated}{err_ampersand_hint}"
                else:
                    detailed_msg = f"{translated}{err_ampersand_hint}"
                
                detailed_errors.append({
                    "line": err_line,
                    "column": err_col,
                    "message": err_msg,
                    "translated_message": translated,
                    "full_line": err_line_full,
                    "display_line": err_line_display,  # –û–±—Ä–µ–∑–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    "is_truncated": err_line_info["truncated"],
                    "line_length": err_line_info.get("line_length", len(err_line_full)),
                    "has_ampersand": has_amp,
                    "detailed_message": detailed_msg
                })
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π –¥–ª—è –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–∏
            if line_num > 0:
                if error_line_display:
                    detailed_message = f"–°—Ç—Ä–æ–∫–∞ {line_num}, –ø–æ–∑–∏—Ü–∏—è {column}: {human_message}{ampersand_hint}\n–ü—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {error_line_display}"
                    if line_info["truncated"]:
                        detailed_message += f"\n(–°—Ç—Ä–æ–∫–∞ –æ–±—Ä–µ–∑–∞–Ω–∞, –ø–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞: {line_info['line_length']} —Å–∏–º–≤–æ–ª–æ–≤)"
                else:
                    detailed_message = f"–°—Ç—Ä–æ–∫–∞ {line_num}, –ø–æ–∑–∏—Ü–∏—è {column}: {human_message}{ampersand_hint}"
            else:
                detailed_message = f"{human_message}{ampersand_hint}"
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—à–∏–±–æ–∫, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if len(all_errors) > 1:
                detailed_message += f"\n\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ XML –æ—à–∏–±–æ–∫: {len(all_errors)}"
            
            return {
                "valid": False,
                "error_code": "XML_SYNTAX_ERROR",
                "message": str(err),  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
                "human_message": detailed_message,  # –ü–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                "line": line_num,
                "column": column,
                "error_text": error_msg,
                "translated_error": human_message,
                "full_line": error_line_full,  # –ü–æ–ª–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –æ—à–∏–±–∫–æ–π
                "display_line": error_line_display,  # –û–±—Ä–µ–∑–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "is_truncated": line_info["truncated"],
                "line_length": line_info.get("line_length", len(error_line_full)),
                "has_ampersand": has_ampersand,
                "all_errors": detailed_errors,  # –í—Å–µ –æ—à–∏–±–∫–∏ —Å –¥–µ—Ç–∞–ª—è–º–∏
                "errors_count": len(all_errors),
                **context_info  # –î–æ–±–∞–≤–ª—è–µ–º error_line –∏ context –µ—Å–ª–∏ –µ—Å—Ç—å
            }
            
        except (FeedDownloadError, FeedValidationError):
            # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∞—à–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–∞–ª—å—à–µ
            raise
        except Exception as e:
            logging.error(f"Unknown error: {e}")
            return {
                "valid": False,
                "error_code": "UNKNOWN_ERROR",
                "message": str(e),
                "human_message": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ XML: {str(e)}"
            }

    def get_tree_object(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ XML –≤ –¥–µ—Ä–µ–≤–æ"""
        try:
            if self.file_content:
                logger.info(f"üì¶ Parsing XML from file_content: {len(self.file_content) / 1024 / 1024:.2f} MB")
                # –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å XML –≤ –ø–∞–º—è—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                self.feed_tree = etree.fromstring(self.file_content)
            else:
                site_data = self.get_url_text()
                logger.info(f"üì¶ Parsing XML from site_data: {len(site_data) / 1024 / 1024:.2f} MB ({len(site_data)} characters)")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                if 'UTF-8' in site_data or 'utf-8' in site_data:
                    encoding = 'utf-8'
                elif 'windows-1251' in site_data:
                    encoding = 'cp1251'
                else:
                    encoding = 'utf-8'
                
                logger.info(f"üîÑ Converting to bytes with encoding: {encoding}")
                xml_bytes = bytes(site_data, encoding=encoding)
                logger.info(f"üì¶ XML bytes size: {len(xml_bytes) / 1024 / 1024:.2f} MB")
                
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º site_data —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                del site_data
                logger.info(f"üóëÔ∏è Freed site_data from memory")
                
                logger.info(f"üå≥ Starting XML parsing (this may take a while for large files)...")
                logger.info(f"‚è±Ô∏è Parsing started at: {datetime.now().isoformat()}")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º BytesIO –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
                from io import BytesIO
                xml_buffer = BytesIO(xml_bytes)
                self.feed_tree = etree.parse(xml_buffer).getroot()
                xml_buffer.close()
                
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
                del xml_bytes
                logger.info(f"üóëÔ∏è Freed xml_bytes from memory")
                
                logger.info(f"‚úÖ XML parsing successful!")
                logger.info(f"‚è±Ô∏è Parsing completed at: {datetime.now().isoformat()}")
            
            logger.info(f"üîß Running check_spelling...")
            self.check_spelling()
            logger.info(f"‚úÖ check_spelling completed")
        except MemoryError as e:
            logger.error(f"‚ùå Memory error during XML parsing: {e}")
            logger.error(f"‚ùå File size: {len(self.file_content) / 1024 / 1024:.2f} MB" if self.file_content else "‚ùå Site data size: large")
            raise FeedValidationError(
                message="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ XML —Ñ–∞–π–ª–∞. –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π.",
                validation_results={
                    "parsing_error": True,
                    "error_type": "MemoryError",
                    "error_message": str(e),
                    "suggestion": "–£–≤–µ–ª–∏—á—å—Ç–µ –ª–∏–º–∏—Ç—ã –ø–∞–º—è—Ç–∏ –≤ Kubernetes –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –ª–µ–≥–∫–∏–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∏–¥–∞"
                }
            )
        except Exception as e:
            logger.error(f"‚ùå Error parsing XML tree: {e}")
            logger.error(f"‚ùå Error type: {type(e).__name__}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            raise

    def check_spelling(self):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–≥–æ–≤"""
        for cat_id in self.feed_tree.iter('categoryid'):
            cat_id.tag = 'categoryId'
        
        cats_with_parents = [
            category for category in self.feed_tree.iter('category') 
            if 'parentid' in category.attrib
        ]
        
        for elem in cats_with_parents:
            elem.attrib['parentId'] = elem.attrib['parentid']
            del elem.attrib['parentid']

    def calculate_category_tree_depth(self):
        """–†–∞—Å—á–µ—Ç –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_hierarchy = {}
        for category in self.site_categories:
            category_id = category.attrib.get('id')
            parent_id = category.attrib.get('parentId', None)
            if parent_id:
                if parent_id not in category_hierarchy:
                    category_hierarchy[parent_id] = []
                category_hierarchy[parent_id].append(category_id)

        def calculate_depth(node_id, depth=1):
            if node_id not in category_hierarchy:
                return depth
            else:
                return max(calculate_depth(child_id, depth + 1) for child_id in category_hierarchy[node_id])

        root_categories = [
            category.attrib['id'] for category in self.site_categories 
            if 'parentId' not in category.attrib
        ]
        
        try:
            max_depth = max(calculate_depth(root_id) for root_id in root_categories) if root_categories else 0
        except:
            max_depth = -1

        return max_depth

    def get_mandatory_requirements(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
        self.site_offers = [elem for elem in self.feed_tree.iter('offer')]
        self.site_categories = [elem for elem in self.feed_tree.iter('category')]

        self.categories_full_info = [
            (category.attrib['id'], category.text, category.attrib.get('parentId', '')) 
            for category in self.site_categories
        ]

        self.categories_ids = [category[0] for category in self.categories_full_info]
        self.categories_names = [category[1] for category in self.categories_full_info]

        offer_ids = []
        
        for offer in self.site_offers:
            offer_id_val = offer.attrib.get('id')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ available
            if 'available' in offer.attrib:
                if offer.attrib['available'] == 'true':
                    self.available_offers_count += 1
                elif offer.attrib['available'] == 'false':
                    self.unavailable_offers_count += 1
                else:
                    self.empty_availability_count += 1
                    self.offers_without_availability.append(offer)
            else:
                self.empty_availability_count += 1
                self.offers_without_availability.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ ID
            if offer_id_val and offer_id_val.strip():
                offer_ids.append(offer_id_val.strip())
            else:
                self.offers_without_id.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ name
            name = offer.find('name')
            name_text = name.text.strip() if name is not None and name.text else None
            if not name_text:
                type_prefix = offer.find('typePrefix')
                vendor_elem = offer.find('vendor')
                model = offer.find('model')
                type_prefix_text = type_prefix.text.strip() if type_prefix is not None and type_prefix.text else None
                vendor_text = vendor_elem.text.strip() if vendor_elem is not None and vendor_elem.text else None
                model_text = model.text.strip() if model is not None and model.text else None

                if not (type_prefix_text and vendor_text and model_text):
                    self.offers_without_name.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ vendor
            vendor_elem = offer.find('vendor')
            vendor_text = vendor_elem.text.strip() if vendor_elem is not None and vendor_elem.text else ''
            if vendor_text:
                self.brands.add(vendor_text)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ url
            url = offer.find('url')
            url_text = url.text.strip() if url is not None and url.text else None
            if not url_text:
                self.offers_without_link.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ price
            price = offer.find('price')
            price_text = price.text.strip() if price is not None and price.text else None
            if not price_text:
                self.offers_price_issues.append(offer)
            else:
                try:
                    price_value = float(price_text)
                    if price_value <= 0:
                        self.offers_price_issues.append(offer)
                except ValueError:
                    self.offers_price_issues.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ categoryId
            categories_parent = offer.find('categories')
            categories = []
            if categories_parent is not None:
                for cat in categories_parent.findall('categoryId'):
                    cat_text = cat.text.strip() if cat is not None and cat.text else None
                    if cat_text:
                        categories.append(cat_text)
            else:
                top_level_cats = offer.findall('categoryId')
                if top_level_cats:
                    for cat in top_level_cats:
                        cat_text = cat.text.strip() if cat is not None and cat.text else None
                        if cat_text:
                            categories.append(cat_text)

            if not categories:
                self.offers_without_category.append(offer)
            else:
                for cat in categories:
                    if cat not in self.categories_ids:
                        if offer not in self.offers_invalid_category:
                            self.offers_invalid_category.append(offer)

                if len(categories) > 1 and categories_parent is None:
                    self.offers_multiple_categories.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ vendor
            bad_vendor_values = {"null", "–±–µ–∑ –±—Ä–µ–Ω–¥–∞", "–Ω–µ—Ç", "–Ω–µ —É–∫–∞–∑–∞–Ω", "unknown", "n/a", ""}
            if vendor_text.lower() in bad_vendor_values:
                self.offers_vendor_issues.append(offer)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ picture
            picture = offer.find('picture')
            picture_text = picture.text.strip() if picture is not None and picture.text else None
            if not picture_text:
                self.offers_without_image.append(offer)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ID
        ids_count = dict(Counter(offer_ids))
        for iD, count in ids_count.items():
            if count > 1:
                self.duplicate_ids.append((iD, count))

        # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_tree_depth = self.calculate_category_tree_depth()

        return {
            "total_offers": len(self.site_offers),
            "available_offers": self.available_offers_count,
            "unavailable_offers": self.unavailable_offers_count,
            "total_categories": len(self.site_categories),
            "category_tree_depth": category_tree_depth,
            "brands_count": len(self.brands),
            "problems": {
                "missing_id": len(self.offers_without_id),
                "missing_availability": self.empty_availability_count,
                "missing_name": len(self.offers_without_name),
                "missing_link": len(self.offers_without_link),
                "price_issues": len(self.offers_price_issues),
                "missing_category": len(self.offers_without_category),
                "invalid_category": len(self.offers_invalid_category),
                "multiple_categories": len(self.offers_multiple_categories),
                "vendor_issues": len(self.offers_vendor_issues),
                "missing_image": len(self.offers_without_image),
            },
            "duplicate_ids": self.duplicate_ids,
        }

    def check_category_issues(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
        all_offers_categories = [
            category.text for offer in self.site_offers 
            for category in offer.iter('categoryId') 
            if len(list(offer.iter('categoryId'))) != 0 and category.text != ''
        ]
        
        all_parents_categories = [
            category.attrib['parentId'] for category in self.site_categories 
            if 'parentId' in category.attrib
        ]
        
        feed_categories = list(set(all_offers_categories)) + list(set(all_parents_categories))

        # –ü—É—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.empty_categories = [
            (category.attrib['id'], category.text) for category in self.site_categories 
            if category.attrib['id'] not in feed_categories
        ]

        # –î—É–±–ª–∏–∫–∞—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        categories_ids_names = [(category.attrib['id'], category.text) for category in self.site_categories]
        categories_names = [category.text for category in self.site_categories]
        categories_dict = Counter(categories_names)

        self.duplicated_cats = []
        for key, values in categories_dict.items():
            if values > 1:
                for elem in categories_ids_names:
                    if key == elem[1]:
                        self.duplicated_cats.append((elem[0], key))

        # –°–¥–≤–æ–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        long_categories = [
            (category_id, category_name) for category_id, category_name 
            in zip(self.categories_ids, self.categories_names)
            if category_name and (len([cword.strip() for cword in category_name.split()]) > 1)
        ]

        self.dual_categories = [
            (ID, name) for ID, name in long_categories for word in name.split()
            if re.search(r'[,/;&]', word) or word.lower() in {'–∏', 'and'}
        ]

        self.dual_categories = list(set(self.dual_categories))

        return {
            "empty_categories": self.empty_categories,
            "duplicated_categories": self.duplicated_cats,
            "dual_categories": self.dual_categories,
        }

    def get_offer_details(self, offer):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π —Ç–æ–≤–∞—Ä–∞"""
        offer_id = offer.attrib.get('id', '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ID')
        name_elem = offer.find('name')
        offer_name = name_elem.text.strip() if name_elem is not None and name_elem.text else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
        url_elem = offer.find('url')
        offer_url = url_elem.text.strip() if url_elem is not None and url_elem.text else '–°—Å—ã–ª–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
        price_elem = offer.find('price')
        offer_price = price_elem.text.strip() if price_elem is not None and price_elem.text else '–¶–µ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'

        # –ü–æ–ª—É—á–∞–µ–º –±—Ä–µ–Ω–¥ (vendor)
        vendor_elem = offer.find('vendor')
        vendor = vendor_elem.text.strip() if vendor_elem is not None and vendor_elem.text else '–ë—Ä–µ–Ω–¥ –Ω–µ —É–∫–∞–∑–∞–Ω'

        # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = []
        categories_parent = offer.find('categories')
        if categories_parent is not None:
            for cat in categories_parent.findall('categoryId'):
                cat_text = cat.text.strip() if cat is not None and cat.text else None
                if cat_text:
                    categories.append(cat_text)
        else:
            top_level_cats = offer.findall('categoryId')
            if top_level_cats:
                for cat in top_level_cats:
                    cat_text = cat.text.strip() if cat is not None and cat.text else None
                    if cat_text:
                        categories.append(cat_text)

        category_display = ', '.join(categories) if categories else '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

        return {
            "id": offer_id,
            "name": offer_name,
            "url": offer_url,
            "price": offer_price,
            "vendor": vendor,
            "categories": category_display,
        }

    def get_problematic_offers(self, problem_type: ProblemType):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ —Ç–∏–ø—É"""
        problem_mapping = {
            ProblemType.MISSING_ID: self.offers_without_id,
            ProblemType.MISSING_AVAILABLE: self.offers_without_availability,
            ProblemType.MISSING_NAME: self.offers_without_name,
            ProblemType.MISSING_LINK: self.offers_without_link,
            ProblemType.PRICE_ISSUES: self.offers_price_issues,
            ProblemType.MISSING_CATEGORY: self.offers_without_category,
            ProblemType.INVALID_CATEGORY: self.offers_invalid_category,
            ProblemType.MULTIPLE_CATEGORIES: self.offers_multiple_categories,
            ProblemType.MISSING_VENDOR: self.offers_vendor_issues,
            ProblemType.MISSING_IMAGE: self.offers_without_image,
        }
        
        offers = problem_mapping.get(problem_type, [])
        return [self.get_offer_details(offer) for offer in offers]

    def build_category_tree(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–æ–¥–∏—Ç–µ–ª–µ–π"""
        tree = []
        orphaned_categories = []
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories_dict = {}
        for cat_id, cat_name, parent_id in self.categories_full_info:
            categories_dict[cat_id] = {
                "id": cat_id,
                "name": cat_name,
                "parent_id": parent_id,
                "children": [],
            }
        
        # –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –∏ –Ω–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–æ–¥–∏—Ç–µ–ª—è–º–∏
        root_categories = []
        for cat_id, cat_info in categories_dict.items():
            parent_id = cat_info["parent_id"]
            
            if not parent_id:
                # –ö–æ—Ä–Ω–µ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                root_categories.append(cat_info)
            elif parent_id in categories_dict:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –¥–æ—á–µ—Ä–Ω—é—é –∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π
                categories_dict[parent_id]["children"].append(cat_info)
            else:
                # –†–æ–¥–∏—Ç–µ–ª—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                orphaned_categories.append({
                    "id": cat_id,
                    "name": cat_info["name"],
                    "missing_parent_id": parent_id,
                })
        
        return {
            "tree": root_categories,
            "orphaned_categories": orphaned_categories,
            "total_categories": len(self.categories_full_info),
        }
    
    def validate_params(self):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–≥–æ–≤ <param> - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞—Ç—Ä–∏–±—É—Ç–∞ name
        –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∏–º–µ–Ω–µ–º (–∏–≥–Ω–æ—Ä–∏—Ä—É—è unit)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ –∏ –ø–æ–ª–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–µ –¥–µ—Ä–µ–≤–æ - —ç—Ç–æ —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±
        """
        errors = []
        duplicate_errors = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–µ –¥–µ—Ä–µ–≤–æ (–æ—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–± - —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π)
        if self.feed_tree is not None:
            total_params_checked = 0
            invalid_params_found = 0
            
            logger.info(f"üå≥ Checking feed_tree for param tags...")
            
            offer_count = 0
            for offer in self.feed_tree.iter('offer'):
                offer_count += 1
                try:
                    params = offer.findall('.//param')
                    total_params_checked += len(params)
                    
                    if offer_count % 1000 == 0:
                        logger.info(f"  Processed {offer_count} offers, found {len(errors)} missing name errors, {len(duplicate_errors)} duplicate errors so far...")
                    
                    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ –∏–º–µ–Ω–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—é (–∏–≥–Ω–æ—Ä–∏—Ä—É—è unit)
                    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {param_name: {param_value: [list of param elements]}}
                    param_names_seen = {}
                    
                    for param in params:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ name
                        # param.attrib - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                        has_name_attr = 'name' in param.attrib
                        param_name = param.attrib.get('name', '').strip() if has_name_attr else ''
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        if total_params_checked <= 10:
                            param_xml = etree.tostring(param, encoding='unicode', pretty_print=False).strip()
                            logger.info(f"  Param #{total_params_checked}: has_name={has_name_attr}, name='{param_name}', xml={param_xml[:150]}")
                        
                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ –Ω–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ name –∏–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π - —ç—Ç–æ –æ—à–∏–±–∫–∞
                        if not param_name:
                            invalid_params_found += 1
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sourceline –∏–∑ lxml –¥–ª—è –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
                            line_num = None
                            if hasattr(param, 'sourceline') and param.sourceline:
                                line_num = param.sourceline
                            
                            param_xml = etree.tostring(param, encoding='unicode', pretty_print=False).strip()
                            
                            error_info = {
                                "error": "–¢–µ–≥ <param> –±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ name –∏–ª–∏ —Å –ø—É—Å—Ç—ã–º name",
                                "param_content": param_xml,
                            }
                            if line_num:
                                error_info["line_number"] = line_num
                                error_info["full_line"] = param_xml
                                error_info["message"] = f"–°—Ç—Ä–æ–∫–∞ {line_num}: –¢–µ–≥ <param> –±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ name –∏–ª–∏ —Å –ø—É—Å—Ç—ã–º name"
                            else:
                                error_info["message"] = "–¢–µ–≥ <param> –±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ name –∏–ª–∏ —Å –ø—É—Å—Ç—ã–º name"
                                error_info["full_line"] = param_xml
                            errors.append(error_info)
                        else:
                            # –ü–∞—Ä–∞–º–µ—Ç—Ä —Å –≤–∞–ª–∏–¥–Ω—ã–º –∏–º–µ–Ω–µ–º - –ø–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ (–∏–≥–Ω–æ—Ä–∏—Ä—É—è unit)
                            param_value = (param.text or '').strip()
                            
                            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—é (unit –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º)
                            if param_name not in param_names_seen:
                                param_names_seen[param_name] = {}
                            
                            if param_value not in param_names_seen[param_name]:
                                param_names_seen[param_name][param_value] = []
                            
                            param_names_seen[param_name][param_value].append(param)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ offer
                    # –î—É–±–ª–∏–∫–∞—Ç = –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∏–º—è + –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (unit –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º)
                    for param_name, value_groups in param_names_seen.items():
                        for param_value, param_list in value_groups.items():
                            if len(param_list) > 1:
                                # –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç - –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∏–º—è –∏ –∑–Ω–∞—á–µ–Ω–∏–µ (–∏–≥–Ω–æ—Ä–∏—Ä—É—è unit)
                                # –°–æ–∑–¥–∞–µ–º –æ—à–∏–±–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–∞ (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ)
                                for i, duplicate_param in enumerate(param_list[1:], 1):
                                    line_num = None
                                    if hasattr(duplicate_param, 'sourceline') and duplicate_param.sourceline:
                                        line_num = duplicate_param.sourceline
                                    
                                    param_xml = etree.tostring(duplicate_param, encoding='unicode', pretty_print=False).strip()
                                    
                                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    all_duplicates = []
                                    for dup_param in param_list:
                                        dup_line = dup_param.sourceline if hasattr(dup_param, 'sourceline') and dup_param.sourceline else None
                                        dup_xml = etree.tostring(dup_param, encoding='unicode', pretty_print=False).strip()
                                        dup_unit = dup_param.attrib.get('unit', '').strip()
                                        all_duplicates.append({
                                            "line_number": dup_line,
                                            "full_line": dup_xml,
                                            "unit": dup_unit if dup_unit else None
                                        })
                                    
                                    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º unit (–µ—Å–ª–∏ –µ—Å—Ç—å)
                                    units_info = []
                                    for dup_param in param_list:
                                        dup_unit = dup_param.attrib.get('unit', '').strip()
                                        if dup_unit:
                                            units_info.append(dup_unit)
                                    
                                    if units_info:
                                        unique_units = list(set(units_info))
                                        if len(unique_units) > 1:
                                            units_str = f" (—Å —Ä–∞–∑–Ω—ã–º–∏ unit: {', '.join(unique_units)})"
                                        elif len(unique_units) == 1:
                                            units_str = f" (unit=\"{unique_units[0]}\")"
                                        else:
                                            units_str = ""
                                    else:
                                        units_str = ""
                                    
                                    error_info = {
                                        "error": f"–î—É–±–ª–∏–∫–∞—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ '{param_name}' —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º '{param_value}' (–∏–≥–Ω–æ—Ä–∏—Ä—É—è unit)",
                                        "param_name": param_name,
                                        "param_value": param_value,
                                        "param_content": param_xml,
                                        "duplicate_count": len(param_list),
                                        "all_duplicates": all_duplicates,
                                    }
                                    if line_num:
                                        error_info["line_number"] = line_num
                                        error_info["full_line"] = param_xml
                                        error_info["message"] = f"–°—Ç—Ä–æ–∫–∞ {line_num}: –î—É–±–ª–∏–∫–∞—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ '{param_name}' —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º '{param_value}'{units_str} (–Ω–∞–π–¥–µ–Ω–æ {len(param_list)} —Ä–∞–∑)"
                                    else:
                                        error_info["message"] = f"–î—É–±–ª–∏–∫–∞—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ '{param_name}' —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º '{param_value}'{units_str} (–Ω–∞–π–¥–µ–Ω–æ {len(param_list)} —Ä–∞–∑)"
                                        error_info["full_line"] = param_xml
                                    duplicate_errors.append(error_info)
                except Exception as e:
                    # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–¥–Ω–æ–≥–æ offer, –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                    logger.warning(f"‚ö†Ô∏è Error processing offer params: {e}", exc_info=True)
                    continue
            
            logger.info(f"üìä Validated {total_params_checked} param tags in {offer_count} offers, found {invalid_params_found} invalid, {len(duplicate_errors)} duplicates")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ name –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã)
            all_errors = errors + duplicate_errors
            
            if all_errors:
                logger.warning(f"‚ö†Ô∏è Found {len(errors)} missing name errors and {len(duplicate_errors)} duplicate errors")
                logger.warning(f"‚ö†Ô∏è Total errors collected: {len(all_errors)}")
                if errors:
                    logger.warning(f"   First missing name error: {errors[0].get('message', 'N/A')}")
                if duplicate_errors:
                    logger.warning(f"   First duplicate error: {duplicate_errors[0].get('message', 'N/A')}")
                    logger.warning(f"   Last duplicate error: {duplicate_errors[-1].get('message', 'N/A') if duplicate_errors else 'N/A'}")
            else:
                logger.info(f"‚úÖ All {total_params_checked} param tags have valid name attribute and no duplicates")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –≤–º–µ—Å—Ç–µ (–Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É)
            logger.info(f"üîç Returning {len(all_errors)} total errors from validate_params")
            return all_errors
        
        return errors
    
    def analyze_parameters(self):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤"""
        total_params = 0
        offers_with_params = 0
        offers_without_params = 0
        
        for offer in self.site_offers:
            params = offer.findall('.//param')
            param_count = len(params)
            
            total_params += param_count
            
            if param_count > 0:
                offers_with_params += 1
            else:
                offers_without_params += 1
        
        total_offers = len(self.site_offers)
        avg_params = total_params / total_offers if total_offers > 0 else 0
        
        has_warning = offers_without_params > 0
        
        return {
            "total_params": total_params,
            "total_offers": total_offers,
            "avg_params_per_offer": round(avg_params, 2),
            "offers_with_params": offers_with_params,
            "offers_without_params": offers_without_params,
            "has_no_params_warning": has_warning,
        }
    
    def analyze_attributes(self):
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ñ–∏–¥–µ"""
        # –ê–Ω–∞–ª–∏–∑ param —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        param_analysis = defaultdict(lambda: {"count": 0, "values": Counter()})
        
        for offer in self.site_offers:
            params = offer.findall('.//param')
            for param in params:
                param_name = param.attrib.get('name', '').strip()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑ name - –æ–Ω–∏ —É–∂–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–ª–æ–≤–ª–µ–Ω—ã –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
                # –ù–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
                if not param_name:
                    logger.warning(f"‚ö†Ô∏è Found param without name in analyze_attributes (should have been caught by validation)")
                    continue
                
                param_value = param.text or ''
                param_unit = param.attrib.get('unit', '')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –µ–¥–∏–Ω–∏—Ü–µ–π –∏–∑–º–µ—Ä–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                full_value = f"{param_value} {param_unit}".strip() if param_unit else param_value
                
                param_analysis[param_name]["count"] += 1
                param_analysis[param_name]["values"][full_value] += 1
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ offer (–Ω–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        offer_tags_analysis = defaultdict(lambda: {"count": 0, "values": Counter()})
        
        # –°–ø–∏—Å–æ–∫ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö —Ç–µ–≥–æ–≤
        interesting_tags = [
            'vendor', 'model', 'vendorCode', 'barcode', 'country_of_origin',
            'delivery', 'pickup', 'store', 'manufacturer_warranty',
            'age', 'weight', 'dimensions', 'sales_notes',
        ]
        
        for offer in self.site_offers:
            for tag_name in interesting_tags:
                tag_elements = offer.findall(f'.//{tag_name}')
                for tag_elem in tag_elements:
                    tag_value = tag_elem.text or ''
                    if tag_value:
                        offer_tags_analysis[tag_name]["count"] += 1
                        offer_tags_analysis[tag_name]["values"][tag_value] += 1
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è param
        formatted_params = []
        for param_name, data in sorted(param_analysis.items(), key=lambda x: x[1]["count"], reverse=True):
            # –ë–µ—Ä–µ–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            top_values = data["values"].most_common(10)
            formatted_params.append({
                "name": param_name,
                "total_count": data["count"],
                "unique_values_count": len(data["values"]),
                "top_values": [{"value": v, "count": c} for v, c in top_values],
            })
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ–≥–æ–≤
        formatted_tags = []
        for tag_name, data in sorted(offer_tags_analysis.items(), key=lambda x: x[1]["count"], reverse=True):
            if data["count"] > 0:  # –¢–æ–ª—å–∫–æ —Ç–µ–≥–∏ –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ñ–∏–¥–µ
                top_values = data["values"].most_common(10)
                formatted_tags.append({
                    "name": tag_name,
                    "total_count": data["count"],
                    "unique_values_count": len(data["values"]),
                    "top_values": [{"value": v, "count": c} for v, c in top_values],
                })
        
        return {
            "params": formatted_params[:50],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 50 —Å–∞–º—ã–º–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏
            "offer_tags": formatted_tags,
            "total_param_types": len(param_analysis),
            "total_offer_tag_types": len([t for t in offer_tags_analysis.values() if t["count"] > 0]),
        }

    def run_full_check(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∏–¥–∞"""
        result = {}

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ XML (–±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞)
        syntax_check = self.check_xml_syntax()
        result["syntax"] = syntax_check

        # 2. –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ä–µ–≤–∞ (–ø—Ä–æ–±—É–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å XML –æ—à–∏–±–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è recover=True)
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –¥—Ä—É–≥–∏—Ö –≤–µ—â–µ–π, –¥–∞–∂–µ –µ—Å–ª–∏ XML –∏–º–µ–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
        try:
            self.get_tree_object()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not parse XML tree (XML syntax errors may prevent parsing): {e}")
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤–æ, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥—Ä—É–≥–∏—Ö –≤–µ—â–µ–π
            # (—Ö–æ—Ç—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–≥—É—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –¥–µ—Ä–µ–≤–∞)
            if not syntax_check.get("valid", True):
                # –ï—Å–ª–∏ XML –Ω–µ–≤–∞–ª–∏–¥–µ–Ω –∏ –¥–µ—Ä–µ–≤–æ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å XML –æ—à–∏–±–∫–∞–º–∏
                # –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
                logger.warning("‚ö†Ô∏è XML syntax errors found, but attempting to continue with other checks...")
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å recover=True –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤–∞
                try:
                    if self.file_content:
                        parser = etree.XMLParser(recover=True)
                        self.feed_tree = etree.fromstring(self.file_content, parser=parser)
                    else:
                        site_data = self.get_url_text()
                        if 'UTF-8' in site_data or 'utf-8' in site_data:
                            encoding = 'utf-8'
                        elif 'windows-1251' in site_data:
                            encoding = 'cp1251'
                        else:
                            encoding = 'utf-8'
                        xml_bytes = bytes(site_data, encoding=encoding)
                        parser = etree.XMLParser(recover=True)
                        self.feed_tree = etree.fromstring(xml_bytes, parser=parser)
                    logger.info("‚úÖ Successfully parsed XML with recover=True, continuing checks...")
                except Exception as recover_error:
                    logger.error(f"‚ùå Could not recover XML tree: {recover_error}")
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ XML –æ—à–∏–±–∫–∏
                    return result
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º site_offers –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–Ω—É–∂–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –¥–µ—Ä–µ–≤–µ)
        if self.feed_tree is not None:
            self.site_offers = [elem for elem in self.feed_tree.iter('offer')]

        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É)
        # –≠—Ç–æ –±–∏–∑–Ω–µ—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("=" * 80)
        logger.info("üîç STARTING PARAM VALIDATION (COLLECTING ALL ERRORS)")
        logger.info("=" * 80)
        logger.info(f"üì¶ Offers count: {len(self.site_offers) if self.site_offers else 0}")
        logger.info(f"üå≥ Feed tree is None: {self.feed_tree is None}")
        
        try:
            param_validation_errors = self.validate_params()
            logger.info("=" * 80)
            logger.info(f"üîç VALIDATION COMPLETE: Found {len(param_validation_errors)} errors")
            logger.info("=" * 80)
        except Exception as e:
            logger.error(f"‚ùå ERROR in validate_params: {e}", exc_info=True)
            param_validation_errors = []
            logger.warning("‚ö†Ô∏è Continuing with empty param validation errors list")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—à–∏–±–∫–∏ –Ω–∞ —Ç–∏–ø—ã
        missing_name_errors = []
        duplicate_errors = []
        
        for error in param_validation_errors:
            if "–±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ name" in str(error.get("error", "")):
                missing_name_errors.append(error)
            else:
                duplicate_errors.append(error)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        if missing_name_errors:
            self.invalid_param_errors = missing_name_errors
        if duplicate_errors:
            self.duplicate_param_errors = duplicate_errors
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏
        if param_validation_errors:
            logger.warning(f"‚ö†Ô∏è Found {len(missing_name_errors)} missing name errors and {len(duplicate_errors)} duplicate errors")
            
            # –ü–æ–º–µ—á–∞–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∫–∞–∫ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
            result["syntax"]["valid"] = False
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–∞—Ö
            error_messages = []
            if missing_name_errors:
                error_messages.append(f"{len(missing_name_errors)} —Ç–µ–≥–æ–≤ <param> –±–µ–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ name")
            if duplicate_errors:
                error_messages.append(f"{len(duplicate_errors)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            
            result["syntax"]["error_code"] = "PARAM_VALIDATION_ERROR" if missing_name_errors else "PARAM_DUPLICATE_ERROR"
            result["syntax"]["message"] = f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {', '.join(error_messages)}"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–æ–π
            first_error = param_validation_errors[0]
            if first_error.get("line_number"):
                result["syntax"]["human_message"] = f"‚ùå –°—Ç—Ä–æ–∫–∞ {first_error['line_number']}: {first_error.get('message', '–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')}"
                result["syntax"]["line"] = first_error["line_number"]
                result["syntax"]["error_line"] = first_error.get("full_line", "")
            else:
                result["syntax"]["human_message"] = f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {', '.join(error_messages)}"
            
            result["syntax"]["translated_error"] = "–û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Å–µ —Ç–µ–≥–∏ <param>"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –æ—à–∏–±–∫–∏ (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
            result["syntax"]["param_validation_errors"] = param_validation_errors  # –í–°–ï –æ—à–∏–±–∫–∏
            result["syntax"]["errors_count"] = len(param_validation_errors)
            result["syntax"]["missing_name_errors"] = missing_name_errors  # –í–°–ï –æ—à–∏–±–∫–∏
            result["syntax"]["duplicate_param_errors"] = duplicate_errors  # –í–°–ï –æ—à–∏–±–∫–∏
            result["syntax"]["missing_name_count"] = len(missing_name_errors)
            result["syntax"]["duplicate_count"] = len(duplicate_errors)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.info(f"üìä Saving to result: {len(param_validation_errors)} total errors, {len(missing_name_errors)} missing name, {len(duplicate_errors)} duplicates")
            
            logger.warning(f"‚ö†Ô∏è Param validation found errors, but continuing with full check...")
        else:
            logger.info("‚úÖ Param validation passed")

        # 4. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        try:
            mandatory = self.get_mandatory_requirements()
            result["mandatory"] = mandatory
        except Exception as e:
            logger.error(f"‚ùå Error in get_mandatory_requirements: {e}", exc_info=True)
            result["mandatory"] = {"error": str(e)}

        # 5. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞—Ö
        try:
            problematic_offers = {
                "missing_id": self.get_problematic_offers(ProblemType.MISSING_ID),
                "missing_availability": self.get_problematic_offers(ProblemType.MISSING_AVAILABLE),
                "missing_name": self.get_problematic_offers(ProblemType.MISSING_NAME),
                "missing_link": self.get_problematic_offers(ProblemType.MISSING_LINK),
                "price_issues": self.get_problematic_offers(ProblemType.PRICE_ISSUES),
                "missing_category": self.get_problematic_offers(ProblemType.MISSING_CATEGORY),
                "invalid_category": self.get_problematic_offers(ProblemType.INVALID_CATEGORY),
                "multiple_categories": self.get_problematic_offers(ProblemType.MULTIPLE_CATEGORIES),
                "vendor_issues": self.get_problematic_offers(ProblemType.MISSING_VENDOR),
                "missing_image": self.get_problematic_offers(ProblemType.MISSING_IMAGE),
            }
            result["problematic_offers"] = problematic_offers
        except Exception as e:
            logger.error(f"‚ùå Error in get_problematic_offers: {e}", exc_info=True)
            result["problematic_offers"] = {"error": str(e)}

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        try:
            categories = self.check_category_issues()
            result["categories"] = categories
        except Exception as e:
            logger.error(f"‚ùå Error in check_category_issues: {e}", exc_info=True)
            result["categories"] = {"error": str(e)}
        
        # 7. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        try:
            logger.info("üå≥ Building category tree...")
            category_tree = self.build_category_tree()
            result["category_tree"] = category_tree
            logger.info(f"‚úÖ Category tree built: {len(category_tree['tree'])} root categories, {len(category_tree['orphaned_categories'])} orphaned")
        except Exception as e:
            logger.error(f"‚ùå Error in build_category_tree: {e}", exc_info=True)
            result["category_tree"] = {"error": str(e)}
        
        # 8. –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        try:
            logger.info("üìà Analyzing parameters...")
            params_stats = self.analyze_parameters()
            result["params_stats"] = params_stats
            logger.info(f"‚úÖ Parameters analyzed: avg {params_stats['avg_params_per_offer']} params per offer")
        except Exception as e:
            logger.error(f"‚ùå Error in analyze_parameters: {e}", exc_info=True)
            result["params_stats"] = {"error": str(e)}
        
        # 9. –ê–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏ –∑–Ω–∞—á–µ–Ω–∏–π
        try:
            logger.info("üè∑Ô∏è Analyzing attributes...")
            attributes_analysis = self.analyze_attributes()
            result["attributes_analysis"] = attributes_analysis
            logger.info(f"‚úÖ Attributes analyzed: {len(attributes_analysis['params'])} param types, {len(attributes_analysis['offer_tags'])} tag types")
        except Exception as e:
            logger.error(f"‚ùå Error in analyze_attributes: {e}", exc_info=True)
            result["attributes_analysis"] = {"error": str(e)}

        return result

